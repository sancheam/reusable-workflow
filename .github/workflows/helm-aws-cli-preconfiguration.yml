name: Helm & AWS CLI Preconfiguration

on:
  workflow_call:
    secrets:
      ACCOUNT_ID_NONPROD:
        required: true
      AWS_ACCESS_KEY_ID_NONPROD:
        required: true
      AWS_SECRET_ACCESS_KEY_NONPROD:
        required: true
      ACCOUNT_ID_PROD:
        required: true
      AWS_ACCESS_KEY_ID_PROD:
        required: true
      AWS_SECRET_ACCESS_KEY_PROD:
        required: true
      AWS_REGION:
        required: true
    inputs:
      environment:
        required: true
        type: string
      eks_name:
        required: true
        type: string
      helm-enabled:
        required: false
        type: boolean
        default: true

jobs:
  helm_preconfiguration:
    runs-on: [os-small-amd64-linux]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set AWS environment
        run: |
          if [[ ${{ inputs.environment }} != "prod" ]]; then
            echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID_NONPROD }}" >> $GITHUB_ENV
            echo "AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY_NONPROD }}" >> $GITHUB_ENV
            echo "AWS_ACCOUNT_ID=${{ secrets.ACCOUNT_ID_NONPROD }}" >> $GITHUB_ENV
            echo "AWS_REGION=${{ secrets.AWS_REGION }}" >> $GITHUB_ENV
          else 
            echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID_PROD }}" >> $GITHUB_ENV
            echo "AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}" >> $GITHUB_ENV
            echo "AWS_ACCOUNT_ID=${{ secrets.ACCOUNT_ID_PROD }}" >> $GITHUB_ENV
            echo "AWS_REGION=${{ secrets.AWS_REGION }}" >> $GITHUB_ENV
          fi

      - name: Configure AWS Credentials 
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/TeamAdmin
          role-skip-session-tagging: true

      - name: Get Caller Identity
        run: |
          aws sts get-caller-identity

      - name: Configure Kube Config
        if: ${{ inputs.helm-enabled }}
        run: |
          aws eks --region ${{ env.AWS_REGION }} update-kubeconfig --name ${{ inputs.eks_name }}

      - name: SSM Session Manager
        if: ${{ inputs.helm-enabled }}
        run: |
          CLUSTER_API=$(aws eks describe-cluster --region ${{ env.AWS_REGION }} --name ${{ inputs.eks_name }} | jq -r '.cluster.endpoint' | awk -F/ '{print $3}')
          sed -ie "s/https:\/\/$CLUSTER_API/https:\/\/$CLUSTER_API:8443/" ~/.kube/config
          sudo -- sh -c "echo 127.0.0.1 $CLUSTER_API >> /etc/hosts"

          INSTANCE_ID=`aws ec2 describe-instances --filters Name=tag:Name,Values=BastionHost Name=instance-state-name,Values=running --query "Reservations[*].Instances[*].InstanceId" --region ${{ env.AWS_REGION }} --output text`
          nohup aws ssm start-session \
          --target $INSTANCE_ID \
          --document-name AWS-StartPortForwardingSessionToRemoteHost \
          --parameters '{"host":["'"$CLUSTER_API"'"],"portNumber":["443"], "localPortNumber":["8443"]}' \
          --region ${{ env.AWS_REGION }} > nohup.out &
          for attempt in {1..20}; do sleep 1; if grep "Waiting for connections..." nohup.out; then echo ready; break; fi; echo waiting...; done
